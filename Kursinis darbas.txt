Skatinamasis mokymas tai masininio mokymosi sritis, kurioje agentas gauna informacija apie aplinka ir ismoksta pasirinkti veiksmus kurie maksimizuoja tikslo funkcija.

Pagrindinis Skatinamojo mokymo pranasumas tai tas, kad palyginus su kitomis masininio mokymo sritimis tai kad agentui nereikia turet jokiu mokymo duomenu pries mokyma, agentas mokosi is patirties, panasiai kaip zmones ar gyvunai

Skatinamasis mokymas paremtas agento ir aplinkos sasaja. Toje sasajoj dalyvauja sios dalys:
Agentas
Aplinka
Veiksmas
Busena
Atlygis

Paprastai pasakius, agentas ir aplinka saveikauja vienas su kitu zingsniu sekoje. Kiekviename zingsnyje, agentas gauna aplinkos busenos reprezentacija ir pasirenka veiksma atsizvelgdamas i tai. Vienu zingsniu veliau, priklausomai nuo savo veiksmu sekos, agentas gauna kazkoki tai skaitini atlygi
ir atsiranda kitoje busenoj

RL algoritmai sakojasi i 2 atskiras sakas - Model-Free ir Model-Based. Model Free algoritmai sakojasi i dar 2 sakas, tai Policy Optimization ir Q-Learning.
Skirtingai nuo Model-Free RL, model-based RL nera konkreciu metodu klusteriu. Yra daug skirtingu budu naudoti tuos modelius

--------------------------------------------------

Unity tai įvairiaplatformis žaidimų kūrimo įrankis, kuriamas bendrovės „Unity Technologies“. Pasinaudojus ML-agents paketu galima apmokyti dirbtinį intelektą pasiremiant giliojo skatinamojo mokymosi principu Unity aplinkoje.

Vietoj to, kad naudociau jau ismokyti modeli, as pabandziau modeli mokyti pats. Mokymui naudojau siuos Hyperparametrus ir algoritma Proximal Policy Optimization kuris yra numatytas mokymo algoritmas ML-Agents aplinkoje.
Agenta pavyko istreniruoti taip, kad jo atlygis po truputi konverguoti iki 100 per epizoda, ir tai yra maksimalus atlygis sioje aplinkoje.

--------------------------------------------------------

OpenAI Gym tai priemoniu rinkinys skirtas skatinamojo mokymosi algoritmu kurimui ir tyrimui. Tai yra biblioteka turinti daug aplinku skirtu testavimui kuriuose galima testuoti skatinamojo mokymosi algoritmus.
OpenAI Gym bando isspresti 2 problemas kurios buvo letina skatinamojo mokymosi tyrima:
1. Geresniu benchmarku poreikis. mokyme su mokytoju procese pazanga leme dideli duomenu rinkiniai. Skatinamojo mokymosi to atitikmuo butu didelis ir ivairus aplinku rinkinys. Taciau esamos Skatinamojo mokymosi atviro kodo aplinkos neturi pakankamai ivairoves ir jas daznai buna sunku pradet naudot.
2. Aplinkos standartizavimo publikacijose trukumas. Mazi skirtumai problemos apibrezime gali drastiskai pakeisti uzduoties sudetinguma. Del to buna sunku atkartoti tyrimo rezultatus ir palyginti juos su kitais tyrimais

OpenAI Gym aplinkos pavyzdyje CartPole bandziau istreniruoti agenta naudojant DQN algoritma su siais hyperparametrais. Siame grafe galime matyti kaip pavyko ji istreniruot ir koki atlygi jis gavo per kiekviena epizoda.